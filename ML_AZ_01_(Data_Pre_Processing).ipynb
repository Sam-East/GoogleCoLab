{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML-AZ-01 (Data Pre-Processing).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO67eyXsnmeqQgTACLOQcBl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sam-East/GoogleCoLab/blob/master/ML_AZ_01_(Data_Pre_Processing).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kz7PP2-W5iQ",
        "colab_type": "text"
      },
      "source": [
        "#**Lectures Folder 02 ~ Data Pre-Processing:**\n",
        "\n",
        "**Data Pre-Processing:**\n",
        "Data preprocessing is a technique which is used to transform the raw data in a useful and efficient format.\n",
        "\n",
        "**Links:**\n",
        "https://www.geeksforgeeks.org/data-preprocessing-in-data-mining/\n",
        "\n",
        "**Step 01:**\n",
        "Include the necessay libraries like numpy, pandas, matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr477u87YTsR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0UzjNLiagy8",
        "colab_type": "text"
      },
      "source": [
        "**Step 02:** Now upload the csv file into pandas framework.\n",
        "\n",
        "First we upload the file.\n",
        "\n",
        "**Links:**https://towardsdatascience.com/3-ways-to-load-csv-files-into-colab-7c14fcbdcb92"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAZpzT8Dar2e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Data file from computer upload\n",
        "'''\n",
        "#create prompt for file upload\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "#Now we add the file uploaded to a pandas obj\n",
        "import io\n",
        "dataset = pd.read_csv(io.BytesIO(uploaded['Data.csv']))\n",
        "'''\n",
        "\n",
        "#Data file from computer From GitHub\n",
        "url = 'https://raw.githubusercontent.com/Sam-East/DataFiles/master/Data.csv'\n",
        "dataset=pd.read_csv(url)\n",
        "\n",
        "\n",
        "#print(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vNhLzl8f4I1",
        "colab_type": "text"
      },
      "source": [
        "**Step 03:**\n",
        "Next we divide the data into **Dependent and Independent** parts\n",
        "\n",
        "In out data set Country, Age & Salary are independent but \"Purchased\" is dependent. So we simply seprate those coloums.\n",
        "\n",
        "**Link:**(use of iloc) https://www.shanelynn.ie/select-pandas-dataframe-rows-and-columns-using-iloc-loc-and-ix/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbimmMF3drLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# iloc[Row ,Coloum] (Slicing done below see link above)\n",
        "X = dataset.iloc[:,:-1].values #(independent values)\n",
        "Y = dataset.iloc[:,3].values #(dependent values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2eBZ2TLvkBC",
        "colab_type": "text"
      },
      "source": [
        "**Step 04:** Now we Fill in the missing values by their means."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TaKl3HeviFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "# (1) We fit the data by applying the SimpleImputer \n",
        "# (2) We then used transform to fill in the missing data in the X. \n",
        "X[:,1:3] = imputer.fit_transform(X[:,1:3])\n",
        "\n",
        "#print(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6whYtM67OCo",
        "colab_type": "text"
      },
      "source": [
        "**Step 05: Encoding Categorical Data:** Categorical variables represent types of data which may be divided into groups. Here categorical data is the data in the \"Country\" and \"Purchase\" coloums.\n",
        "\n",
        "**Link:** http://www.stat.yale.edu/Courses/1997-98/101/catdat.htm\n",
        "\n",
        "-\"Country\" has categories {France, Spain, Germany} \n",
        "\n",
        "-\"Purchase\" has categories {Yes, No}\n",
        "\n",
        "**Encoding:** Since we need to used data into mathematical algorithms. We need to express the categorical data (which is in >>String<< format) into some kind of Numeric data (>>Number<< format), Hence Encoding of categorical data.\n",
        "\n",
        "**A Problem:** A problem here is that since we assign values to strings, those values can get mis-interpret for being some kind of characteristic of the string like if (A=1 and B=2), An algorithm might assue (A < B). which is not the case. \n",
        "\n",
        "**Solution (Dummy Varibles):** We use OneHotEncoder class to solve this, We transform the data into three coloums (Because we have three values there) now each entry is represented by three digits(Bolleans). Now in each of the three boleans we represents every country. If frist row is France the bolean of france will be one the other two will be zero.(This is Called Encoding Dummy Varibles)\n",
        "\n",
        "**Link for ColumnTransformer:** https://stackoverflow.com/questions/59525929/valueerror-shape-mismatch-if-categories-is-an-array-it-has-to-be-of-shape-n\n",
        "\n",
        "\n",
        "For **Dependent variable** we **don't need to use OnehotEncoder** the machine learning algorithm automatically knows\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPX28oKAU2CP",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_0KmgYC7rDp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "labelEncoder_X = LabelEncoder()\n",
        "#1st Column of Categorical Data of the DataSet X\n",
        "X[:,0] = labelEncoder_X.fit_transform(X[:,0]) #Encoding of data onto Numbers\n",
        "oneHotEncoder = ColumnTransformer([('one_hot_encoder',OneHotEncoder(categories = 'auto'),[0])],remainder='passthrough') #Encoding Dummy Variable\n",
        "X = oneHotEncoder.fit_transform(X)\n",
        "\n",
        "#Column of Categorical Data of the DataSet Y\n",
        "#Just need to use LabelEncoder Since the algorithm knows it a dependent varible coloum\n",
        "labelEncoder_Y = LabelEncoder()\n",
        "Y = labelEncoder_Y.fit_transform(Y) #Encoding of data onto Numbers\n",
        "\n",
        "\n",
        "#print(\"Encoded Table of X \\n\",X, \" \\n ----- \\n\")\n",
        "#print(\"Encoded Table of Y \\n\",Y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pd8tkaX7lA1E",
        "colab_type": "text"
      },
      "source": [
        "**Step 06:** Spliting the DataSet Into a **Training Set and a Test set**.\n",
        "\n",
        "The size of test data is generally 0.2(meaning 20%) or 0.25(meaning 25%). This can at most be 0.4(meaning 40%) which is not common. Something like 0.5 (meaning 50%) is not a good strategy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ri0WnvvWlLYz",
        "colab_type": "code",
        "outputId": "a51d95e3-760e-445e-9105-f8f53193c342",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Parameters are the (1) Arrays (DataSet Dependent and Independent parts), (2) Size of testing data.\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\n",
        "\n",
        "\n",
        "'''\n",
        "print(X_train, \"\\n \\n\")\n",
        "print(X_test, \"\\n \\n\")\n",
        "print(Y_train, \"\\n \\n\")\n",
        "print(Y_test, \"\\n \\n\") \n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nprint(X_train, \"\\n \\n\")\\nprint(X_test, \"\\n \\n\")\\nprint(Y_train, \"\\n \\n\")\\nprint(Y_test, \"\\n \\n\") \\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zepCqEpbqeYv",
        "colab_type": "text"
      },
      "source": [
        "**Step 07 - Feature Scaling:** We scale the features present in our dataset. The numeric data in each column can have a lot of difference like in our data \"age\"(that might be between 0-100 mostly) and \"salary\"(that can be in thousands or above like 4k or 10k upto 90k or 1M etc) have a difference in the scaling.\n",
        "\n",
        "**Problem:** Because most of our ML Algorithms are based on Euclidean Distance, And if we compute something on base of Age and Salary column its will be too heavily dominated by the Salary column due to values being large.\n",
        "\n",
        "**Solution:** The solution is to either use:\n",
        "\n",
        "**1.Standardisation =  (x - mean(x)) / standarddeviation(x)**\n",
        "\n",
        "or\n",
        "\n",
        "**2.Normalization = (x - min(x)) / (max(x) - min(x))**\n",
        "\n",
        "**Some Points to Consider:** \n",
        "\n",
        "**1st** thing to conside is that, **Do we need to fit and transform our Dummy Variable?** It depends if we transform them (1)We will have a high prediction accuracy and (2)It will work very fast, but as a down side we will lose our interpretation of the varible (meaning they will be hard for us to see and understand ourself). Over all scaling the Dummy Varibles is a good thing.\n",
        "\n",
        "**2nd** thing is if we are working with a model that doesn't use Euclidean Distance like Decision trees even then Scaling is a good idea because the model will get very fast due to it.\n",
        "\n",
        "**3rd** Do we need to **scale the Dependent Vaiable Y ?** The answer is no this time because this is a classification problem with a category called dependent, but in other datasets/problems when the dependent variable will take a huge range of values we will apply transformation to the Dependent Variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4n8trpGsY5v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "sc_X = StandardScaler()\n",
        "#When we fit_transform the X_train the same fitting happens on the test but we need to apply transformation to it sepratly\n",
        "X_train = sc_X.fit_transform(X_train)\n",
        "X_test =  sc_X.transform(X_test) \n",
        "\n",
        "'''\n",
        "print(X_train , \" \\n \\n\")\n",
        "print(X_test)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}